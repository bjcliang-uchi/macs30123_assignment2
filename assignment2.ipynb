{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, numpy, datetime, dataset, re, pywren\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import pywren\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from scraper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dataset.connect('sqlite:///books.db')\n",
    "base_url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape the pages in the catalogue\n",
    "url = base_url\n",
    "while True:\n",
    "    r = requests.get(url)\n",
    "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    scrape_books(html_soup, url, db)\n",
    "    # Is there a next page?\n",
    "    next_a = html_soup.select('li.next > a')\n",
    "    if not next_a or not next_a[0].get('href'): break\n",
    "    url = urljoin(url, next_a[0].get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serial time spent: 0:09:55.448328\n"
     ]
    }
   ],
   "source": [
    "books = db['books'].find(order_by=['last_seen'])\n",
    "\n",
    "t0 = datetime.now()\n",
    "for book in books:\n",
    "    book_id = book['book_id']\n",
    "    book_url = base_url + 'catalogue/{}'.format(book_id)\n",
    "    r = requests.get(book_url)\n",
    "    r.encoding = 'utf-8'\n",
    "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    scrape_book(html_soup, book_id, db)\n",
    "    db['books'].upsert({'book_id' : book_id,\n",
    "                        'last_seen' : datetime.now()\n",
    "                        }, ['book_id'])\n",
    "\n",
    "print('serial time spent:', datetime.now()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_book_parallel(book_id):\n",
    "    \n",
    "    base_url = 'http://books.toscrape.com/'\n",
    "    book_url = base_url + 'catalogue/{}'.format(book_id) \n",
    "    \n",
    "    r = requests.get(book_url)\n",
    "    r.encoding = 'utf-8'\n",
    "    html_soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    main = html_soup.find(class_='product_main')\n",
    "    \n",
    "    book = {}\n",
    "    book['book_id'] = book_id\n",
    "    book['title'] = main.find('h1').get_text(strip=True)\n",
    "    book['price'] = main.find(class_='price_color').get_text(strip=True)\n",
    "    book['stock'] = main.find(class_='availability').get_text(strip=True)\n",
    "    book['rating'] = ' '.join(main.find(class_='star-rating') \\\n",
    "                        .get('class')).replace('star-rating', '').strip()\n",
    "    book['img'] = html_soup.find(class_='thumbnail').find('img').get('src')\n",
    "    desc = html_soup.find(id='product_description')\n",
    "    book['description'] = ''\n",
    "    if desc: book['description'] = desc.find_next_sibling('p').get_text(strip=True)\n",
    "    book_product_table = html_soup.find(text='Product Information').find_next('table')\n",
    "    for row in book_product_table.find_all('tr'):\n",
    "        header = row.find('th').get_text(strip=True)\n",
    "        header = re.sub('[^a-zA-Z]+', '_', header)\n",
    "        value = row.find('td').get_text(strip=True)\n",
    "        book[header] = value     \n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwex = pywren.default_executor()\n",
    "book_ids = [book['book_id'] for book in db['books']]\n",
    "split = split_list(book_ids, 50) ## 50 urls for each lambda instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel time spent: 0:00:28.147050\n"
     ]
    }
   ],
   "source": [
    "def scrape_helper(id_lst): \n",
    "    return [scrape_book_parallel(book_id) for book_id in id_lst]\n",
    "\n",
    "t0 = datetime.now()\n",
    "futures = pwex.map(scrape_helper, split)\n",
    "result = pywren.get_all_results(futures)\n",
    "print('parallel time spent:', datetime.now()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last transfer time: 0:00:01.812117\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "result = [j for i in result for j in i]\n",
    "for book in result:\n",
    "    db['books'].upsert({'book_id' : book['book_id'],\n",
    "                        'last_seen' : datetime.now()}, ['book_id'])\n",
    "print('last transfer time:', datetime.now()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It takes the original serial version about 10 mins to run, but it takes PyWren only 28+2 = 30 secs. \n",
    "* There is still a significant serial part at the beginning (before we start to scrape), taking about 12.26 secs to transfer the data to each instance.\n",
    "* **the logic behind using a relational database** The traditional databases were flat. This means that the information was stored in one long text file (tab delimited file). The text file makes it difficult to search for specific information or to create reports that include only certain fields from each record. [[1]](https://computer.howstuffworks.com/question599.htm) In this scraping solution we use a relational database because it is faster and safer to for SQL-type databases to add, update or delete rows of data, retrieving subsets of data for transaction processing and analytics applications, and to manage all aspects of the database.[[2]](https://aws.amazon.com/relational-database/)\n",
    "* **We would want to scale up to AWS large-scale database** 1) if the information of each book keeps changing and we want to keep updated with these changes. In this case, we would want a distributed data storage method which maintains availability (every request receives a non-error response) and partition tolerance (the system continues to operate despite network failures). 2) If we want to make the data to be publicly available: S3 provides cost-efficient methods for us to store and publish large scale databse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: MRJOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for book in result:\n",
    "    db['books'].upsert({'book_id' : book['book_id'],\n",
    "                        'desc': book['description'],\n",
    "                        'last_seen' : datetime.now()}, ['book_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /Users/chen.liang/.mrjob.conf\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/1_/vb8z7w9d62s_hq9m0yd097z00000gn/T/mrjob_pb2.chen.liang.20200516.071749.095448\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /var/folders/1_/vb8z7w9d62s_hq9m0yd097z00000gn/T/mrjob_pb2.chen.liang.20200516.071749.095448/output\n",
      "Streaming final output from /var/folders/1_/vb8z7w9d62s_hq9m0yd097z00000gn/T/mrjob_pb2.chen.liang.20200516.071749.095448/output...\n",
      "[[2002, \"with\"], [2147, \"that\"], [2513, \"her\"], [3136, \"is\"], [4348, \"in\"], [6096, \"to\"], [7088, \"a\"], [7882, \"of\"], [8705, \"and\"], [13156, \"the\"]]\tnull\n",
      "Removing temp directory /var/folders/1_/vb8z7w9d62s_hq9m0yd097z00000gn/T/mrjob_pb2.chen.liang.20200516.071749.095448...\n"
     ]
    }
   ],
   "source": [
    "desc = [book['desc'] for book in db['books']]\n",
    "with open(\"book_description.tsv\", \"w\") as file:\n",
    "    for i in desc: file.write(i+'\\n')\n",
    "! python mrjob_pb2.py book_description.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: SNS Notifications\n",
    "\n",
    "See the seperated `problem3 jupyter notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Propose a Final Project Topic\n",
    "* I propose a thesis-related political science research topic that primarily uses PyWren to scrape Twitter data and potentially analyze tweets as well.\n",
    "* The general idea is that I want to construct an expert-based social network for each targeted policy think tank, and try to prove that 1) twitter follower/following network can imply the level of power centralization in an organization like think tanks. 2) Experts' control over information flow, i.e. betweenness centrality, is strongly correlated with their roles within the organization. (See the graph below)\n",
    "<img src=\"heritage.png\" width=\"600\" align=\"center\" />\n",
    "* To collect data, I need to automatically scrape over 600 twitter handles and obtain not only their profile information but also a whole list of their followers and following accounts. It is particularly computationally exhaustive to scrape all follower accounts because we need to sequentially scrape each page which gives 20 follower accounts as well as a cursor that indicates the url of the next page. I am using PyWren to parallel the scraping process; that is, I can scrape the followers of multiple accounts at the same time. By using AWS services, I can also circumvent the IP issues. \n",
    "* Likewise, I will also use PyWren to scrape the profile information and historical tweets for each targeted accounts. Potentially, (if time permits), I am planning to analyze the political stances as implied by the historical tweets with a semantic topic modeling approach. If the dataset is too large, I might use RCC and MPI to multiprocess tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
